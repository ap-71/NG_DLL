{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./DSDZ3\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./DSDZ3\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./DSDZ3\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./DSDZ3\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./DSDZ3\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./DSDZ3\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./DSDZ3\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./DSDZ3\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./DSDZ3\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./DSDZ3\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./DSDZ3\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./DSDZ3\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 'cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE=256\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "\n",
    "train_dataset = datasets.FashionMNIST('./DSDZ3', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.FashionMNIST('./DSDZ3', train=False, transform=transforms.ToTensor(), download=True)\n",
    "train = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "test = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle = False)\n",
    "\n",
    "train_dataset[0][0].shape, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(**kwargs) -> None:\n",
    "    loss_ = kwargs['loss']\n",
    "    trainer_ = kwargs['trainer']\n",
    "    num_epochs_ = kwargs['num_epochs']\n",
    "    model_ = kwargs['model']\n",
    "    train_ = kwargs['train']\n",
    "    test_ = kwargs['test']\n",
    "    device_ = kwargs['device']\n",
    "    \n",
    "    for ep in range(num_epochs_):\n",
    "        train_iters, train_passed  = 0, 0\n",
    "        train_loss, train_acc = 0., 0.\n",
    "        start=time.time()\n",
    "        \n",
    "        model_.train()\n",
    "        for X, y in train_:\n",
    "            X, y = X.to(device_), y.to(device_)\n",
    "            trainer_.zero_grad()\n",
    "            y_pred = model_(X)\n",
    "            l = loss_(y_pred, y)\n",
    "            l.backward()\n",
    "            trainer_.step()\n",
    "            train_loss += l.item()\n",
    "            train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "            train_iters += 1\n",
    "            train_passed += len(X)\n",
    "        \n",
    "        test_iters, test_passed  = 0, 0\n",
    "        test_loss, test_acc = 0., 0.\n",
    "        model_.eval()\n",
    "        for X, y in test_:\n",
    "            X, y = X.to(device_), y.to(device_)\n",
    "            y_pred = model_(X)\n",
    "            l = loss_(y_pred, y)\n",
    "            test_loss += l.item()\n",
    "            test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "            test_iters += 1\n",
    "            test_passed += len(X)\n",
    "            \n",
    "        print(\"ep: {}, taked: {:.3f}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "            ep, time.time() - start, train_loss / train_iters, train_acc / train_passed,\n",
    "            test_loss / test_iters, test_acc / test_passed)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, taked: 27.745, train_loss: 0.655225603884839, train_acc: 0.7569, test_loss: 0.4317518025636673, test_acc: 0.8379\n",
      "ep: 1, taked: 23.119, train_loss: 0.43542046141117174, train_acc: 0.84435, test_loss: 0.3432627562433481, test_acc: 0.8765\n",
      "ep: 2, taked: 23.187, train_loss: 0.38407165129133997, train_acc: 0.86255, test_loss: 0.36668400689959524, test_acc: 0.8639\n",
      "ep: 3, taked: 24.391, train_loss: 0.3572110560346157, train_acc: 0.8717666666666667, test_loss: 0.3281657982617617, test_acc: 0.8837\n",
      "ep: 4, taked: 22.451, train_loss: 0.3423380477631346, train_acc: 0.8775166666666666, test_loss: 0.31921753771603106, test_acc: 0.8849\n",
      "ep: 5, taked: 24.963, train_loss: 0.32783743111377067, train_acc: 0.8828666666666667, test_loss: 0.3057827863842249, test_acc: 0.8884\n",
      "ep: 6, taked: 23.066, train_loss: 0.3250446049456901, train_acc: 0.8838333333333334, test_loss: 0.3027260836213827, test_acc: 0.8913\n",
      "ep: 7, taked: 25.229, train_loss: 0.31814493968131696, train_acc: 0.8850833333333333, test_loss: 0.3101284055039287, test_acc: 0.8865\n",
      "ep: 8, taked: 26.719, train_loss: 0.3122233380662634, train_acc: 0.8871, test_loss: 0.2988518577069044, test_acc: 0.8915\n",
      "ep: 9, taked: 25.404, train_loss: 0.31130792834657306, train_acc: 0.8882666666666666, test_loss: 0.30056961737573146, test_acc: 0.8933\n",
      "ep: 10, taked: 23.441, train_loss: 0.3040518742292485, train_acc: 0.8910833333333333, test_loss: 0.3046079959720373, test_acc: 0.8964\n",
      "ep: 11, taked: 22.767, train_loss: 0.29737837504833303, train_acc: 0.89315, test_loss: 0.32935554534196854, test_acc: 0.8887\n",
      "ep: 12, taked: 24.456, train_loss: 0.30081973874822576, train_acc: 0.8923666666666666, test_loss: 0.35270244628190994, test_acc: 0.8788\n",
      "ep: 13, taked: 23.911, train_loss: 0.2893361786578564, train_acc: 0.89575, test_loss: 0.3199005041271448, test_acc: 0.8923\n",
      "ep: 14, taked: 24.716, train_loss: 0.2928590759952018, train_acc: 0.89465, test_loss: 0.32089953646063807, test_acc: 0.8952\n",
      "ep: 15, taked: 23.218, train_loss: 0.283901391891723, train_acc: 0.89825, test_loss: 0.2955022480338812, test_acc: 0.8959\n",
      "ep: 16, taked: 23.571, train_loss: 0.28221401565886556, train_acc: 0.89935, test_loss: 0.307863712310791, test_acc: 0.8983\n",
      "ep: 17, taked: 23.498, train_loss: 0.2790025658429937, train_acc: 0.9006166666666666, test_loss: 0.3148521978408098, test_acc: 0.8891\n",
      "ep: 18, taked: 22.991, train_loss: 0.28287685037927424, train_acc: 0.90125, test_loss: 0.3204029526561499, test_acc: 0.8935\n",
      "ep: 19, taked: 24.287, train_loss: 0.2699688514496418, train_acc: 0.9023666666666667, test_loss: 0.31436100602149963, test_acc: 0.8998\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=1),\n",
    "    nn.Conv2d(6, 12, kernel_size=4),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(1452, 502),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(502, 51),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(51, 10)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "train_model(\n",
    "    loss=torch.nn.CrossEntropyLoss(),\n",
    "    trainer=torch.optim.Adam(model.parameters(), lr=.01),\n",
    "    num_epochs=20,\n",
    "    model=model,\n",
    "    train=train,\n",
    "    test=test,\n",
    "    device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
